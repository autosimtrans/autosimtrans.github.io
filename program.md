---
layout: main-anchor
title: Program
order: 0
collection: pages_2020
permalink: /program
---


<h1 class='my-center'>Conference Program</h1>
<h4 class='my-center'>(Friday, July 10, or <sup>+1</sup>: Saturday, July 11)</h4>


<style>
	.col-3>img {
		width: 100%;
	}
	table {
		/*table-layout: fixed;*/
		/*width: 80%;*/
		white-space: nowrap;

	}
	h1.my-center, h4.my-center {
		text-align:center;
	}
	th {
		text-align:center;
	}
	td,th {
		font-family: "Times New Roman";
	}
	I {
		font-family: "Times New Roman";
		font-style: italic;
	}
</style>

<table>
<thead>
<tr><th>PDT<br>Pacific Time  </th><th>EDT<br>Eastern Time      </th><th>CET<br>Central European  </th><th>GMT+8<br>Beijing Time    </th><th>                                                                                                                                                                            </th></tr>
</thead>
<tbody>
<tr><td>08:50-09:00          </td><td>11:50-12:00              </td><td>17:50-18:00              </td><td>23:50-00:00              </td><td><I>Opening Remarks</I>                                                                                                                                                      </td></tr>
<tr><td>09:00-11:00          </td><td>12:00-14:00              </td><td>18:00-20:00              </td><td>00:00-02:00 <sup>+1</sup></td><td><b>Session 1 (chair: Liang Huang)</b>                                                                                                                                       </td></tr>
<tr><td>09:00-09:30          </td><td>12:00-12:30              </td><td>18:00-18:30              </td><td>00:00-00:30 <sup>+1</sup></td><td><I><a href="https://autosimtrans.github.io/program#invited-talk-1-by-colin-cherry">Invited Talk 1: Colin Cherry</a></I>                                                     </td></tr>
<tr><td>09:30-10:00          </td><td>12:30-13:00              </td><td>18:30-19:00              </td><td>00:30-01:00 <sup>+1</sup></td><td><I><a href="https://autosimtrans.github.io/program#invited-talk-2-by-barry-slaughter-olsen">Invited Talk 2: Barry Slaughter Olsen</a></I>                                   </td></tr>
<tr><td>10:00-10:30          </td><td>13:00-13:30              </td><td>19:00-19:30              </td><td>01:00-01:30 <sup>+1</sup></td><td><I><a href="https://autosimtrans.github.io/program#invited-talk-3-by-jordan-boyd-graber">Invited Talk 3: Jordan Boyd-Graber</a></I>                                         </td></tr>
<tr><td>10:30-11:00          </td><td>13:30-14:00              </td><td>19:30-20:00              </td><td>01:30-02:00 <sup>+1</sup></td><td><I>Q&A</I>                                                                                                                                                                  </td></tr>
<tr><td>11:00-14:00          </td><td>14:00-17:00              </td><td>20:00-23:00              </td><td>02:00-05:00 <sup>+1</sup></td><td><b>Lunch</b>                                                                                                                                                                </td></tr>
<tr><td>17:00-19:00          </td><td>20:00-22:00              </td><td>02:00-04:00 <sup>+1</sup></td><td>08:00-10:00 <sup>+1</sup></td><td><b>Session 2 (chair: Colin Cherry)</b>                                                                                                                                      </td></tr>
<tr><td>17:00-17:30          </td><td>20:00-20:30              </td><td>02:00-02:30 <sup>+1</sup></td><td>08:00-08:30 <sup>+1</sup></td><td><I><a href="https://autosimtrans.github.io/program#invited-talk-4-by-hua-wu">Invited Talk 4: Hua Wu</a></I>                                                                 </td></tr>
<tr><td>17:30-18:00          </td><td>20:30-21:00              </td><td>02:30-03:00 <sup>+1</sup></td><td>08:30-09:00 <sup>+1</sup></td><td><I><a href="https://autosimtrans.github.io/program#invited-talk-5-by-kay-fan-cheung">Invited Talk 5: Kay-Fan Cheung</a></I>                                                 </td></tr>
<tr><td>18:00-18:30          </td><td>21:00-21:30              </td><td>03:00-03:30 <sup>+1</sup></td><td>09:00-09:30 <sup>+1</sup></td><td><I><a href="https://autosimtrans.github.io/program#invited-talk-6-by-qun-liu">Invited Talk 6: Qun Liu</a></I>                                                               </td></tr>
<tr><td>18:30-19:00          </td><td>21:30-22:00              </td><td>03:30-04:00 <sup>+1</sup></td><td>09:30-10:00 <sup>+1</sup></td><td><I>Q&A</I>                                                                                                                                                                  </td></tr>
<tr><td>19:00-19:30          </td><td>22:00-22:30              </td><td>04:00-04:30 <sup>+1</sup></td><td>10:00-10:30 <sup>+1</sup></td><td><b>Break</b>                                                                                                                                                                </td></tr>
<tr><td>19:30-20:30          </td><td>22:30-23:30              </td><td>04:30-05:30 <sup>+1</sup></td><td>10:30-11:30 <sup>+1</sup></td><td><b>Session 3: Research Paper and System Description (chair: Zhongjun He)</b>                                                                                                </td></tr>
<tr><td>19:30-19:40          </td><td>22:30-22:40              </td><td>04:30-04:40 <sup>+1</sup></td><td>10:30-10:40 <sup>+1</sup></td><td><I>Dynamic Sentence Boundary Detection for Simultaneous Translation</I><br>Ruiqing Zhang and Chuanqiang Zhang                                                               </td></tr>
<tr><td>19:40-19:50          </td><td>22:40-22:50              </td><td>04:40-04:50 <sup>+1</sup></td><td>10:40-10:50 <sup>+1</sup></td><td><I>End-to-End Speech Translation with Adversarial Training</I><br>Xuancai Li, Chen Kehai, Tiejun Zhao and Muyun Yang                                                        </td></tr>
<tr><td>19:50-20:00          </td><td>22:50-23:00              </td><td>04:50-05:00 <sup>+1</sup></td><td>10:50-11:00 <sup>+1</sup></td><td><I>Robust Neural Machine Translation with ASR Errors</I><br>Haiyang Xue, Yang Feng, Shuhao Gu and Wei Chen                                                                  </td></tr>
<tr><td>20:00-20:10          </td><td>23:00-23:10              </td><td>05:00-05:10 <sup>+1</sup></td><td>11:00-11:10 <sup>+1</sup></td><td><I>Improving Autoregressive NMT with Non-Autoregressive Model</I><br>Long Zhou, Jiajun Zhang and Chengqing Zong                                                             </td></tr>
<tr><td>20:10-20:20          </td><td>23:10-23:20              </td><td>05:10-05:20 <sup>+1</sup></td><td>11:10-11:20 <sup>+1</sup></td><td><I>Modeling Discourse Structure for Document-level Neural Machine Translation</I><br>Junxuan Chen, Xiang Li, Jiarui Zhang, Chulun Zhou, Jianwei Cui, Bin Wang and Jinsong Su</td></tr>
<tr><td>20:20-20:30          </td><td>23:20-23:30              </td><td>05:20-05:30 <sup>+1</sup></td><td>11:20-11:30 <sup>+1</sup></td><td><I>BITâ€™s system for the AutoSimTrans 2020</I><br>Minqin Li, Haodong Cheng, Yuanjie Wang, Sijia Zhang, Liting Wu and Yuhang Guo                                              </td></tr>
<tr><td>20:30-21:00          </td><td>23:30-00:00              </td><td>05:30-06:00 <sup>+1</sup></td><td>11:30-12:00 <sup>+1</sup></td><td><I>Q&A</I>                                                                                                                                                                  </td></tr>
<tr><td>21:00-21:10          </td><td>00:00-00:10 <sup>+1</sup></td><td>06:00-06:10 <sup>+1</sup></td><td>12:00-12:10 <sup>+1</sup></td><td><I>Closing Remarks</I>                                                                                                                                                      </td></tr>
</tbody>
</table>
---
### Invited Talk 1 by [Colin Cherry](https://sites.google.com/site/colinacherry)

<div class='container'>
<div class='row'>
	<div class='col-3'>
		<img alt="pic" src="https://sites.google.com/site/colinacherry/home/ColinProfile.jpg?attredirects=0">
	</div>
	<div class='col-9'>
		<b>Title</b>: 
		<br>
		<b>Abstract</b>: 
	</div>
</div>
</div>


---

### Invited Talk 2 by [Barry Slaughter Olsen](https://www.middlebury.edu/institute/people/barry-slaughter-olsen)

<div class='container'>
<div class='row'>
	<div class='col-3'>
		<img alt="pic" src="https://www.middlebury.edu/institute/sites/www.middlebury.edu.institute/files/styles/432x576/public/2019-06/Barry_0.jpg?fv=SevLMSn7&itok=wkJTqnjI">
	</div>
	<div class='col-9'>
		<b>Title</b>: Human Interpreter Training and Practice: Insights for Simultaneous Machine Translation Research
		<br>
		<b>Abstract</b>: Interpreter training and machine translation research are two radically different worlds. Neither understands the other well. Even so, knowing the basic techniques employed by trained simultaneous interpreters to practice their craft can help researchers better comprehend the task of simultaneous machine translation, determine new approaches to that task , and have a clearer understanding of what the potential of the technology may be. In his address, Professor Olsen will provide an overview of the skills and techniques taught in a simultaneous interpreter training program and suggest possible parallels and limitations in their application to simultaneous machine translation.
	</div>
</div>
</div>
 
**Barry Slaughter Olsen** is a veteran conference interpreter and technophile with over twenty-five years of experience interpreting, training interpreters and organizing language services. He is a professor at the Middlebury Institute of International Studies at Monterey ([MIIS](http://www.miis.edu/)) and the Vice-President of Client Success at [KUDO](http://www.kudoway.com/), a multilingual web conferencing platform. He was co-president of [InterpretAmerica](http://www.interpretamerica.com/) from 2009 to 2020. He is a member of the International Association of Conference Interpreters ([AIIC](http://www.aiic.net/)). Barry has been interviewed frequently by international media (CNN,CBC, MSNBC, NPR and PBS) about interpreting and translation. For updates on interpreting, technology and training , follow him on Twitter [@ProfessorOlsen](https://twitter.com/ProfessorOlsen).

---
### Invited Talk 3 by [Jordan Boyd-Graber](http://users.umiacs.umd.edu/~jbg)

<div class='container'>
<div class='row'>
	<div class='col-3'>
		<img alt="pic" src="http://users.umiacs.umd.edu/~jbg/images/jbg.png">
	</div>
	<div class='col-9'>
		<b>Title</b>: Evaluating Human-Computer Simultaneous Interpretation  
		<br>
		<b>Abstract</b>: Human simultaneous interpretation is an amazing feat requiring skill and extensive training.  Computers are simply nowhere close to expert interpreters---but perhaps they can help humans do a task with unique cognitive burdens more effectively.  In this talk, I discuss previous work on computer assistance for human simultaneous interpreters and how it reveals the differences between humans' and computers' comparative skills.  To focus on where computers can best help interpreters, we pilot an evaluation framework to prototype assistance for interpreters with proxy users.  By breaking up interpretation into its constituent pieces, we can both test with a larger user population and pinpoint which assistance techniques are effective when.
	</div>
</div>
</div>

---

### Invited Talk 4 by [Hua Wu](http://research.baidu.com/People/index-view?id=121)

<div class='container'>
<div class='row'>
	<div class='col-3'>
		<img alt="pic" src="http://research.baidu.com/Public/uploads/5ad01e8d5eb46.jpg">
	</div>
	<div class='col-9'>
		<b>Title</b>: Baidu Simultaneous Translation: Research and Applications  
		<br>
		<b>Abstract</b>: Simultaneous translation has been widely studied and used in recent years. In this talk, I will introduce the main challenges of simultaneous translation and our solutions. We proposed methods to get tradeoff between translation quality and latency, such as segmentation models to split ASR output into information units, the incremental TTS to reduce time latency. We also proposed end-to-end models that jointly learns ASR and speech-to-text translation. In order to facilitate research on simultaneous translation, we released BSTC, a Chinese-English simultaneous translation data set containing about 70 hours of Chinese speech audio, human transcripts, ASR results and English translations. In the last part of this talk, I will also introduce the applications of our simultaneous translation system, such as online meetings, lectures, and plugins for video translation.
	</div>
</div>
</div>

**Hua Wu** is the Chief Scientist of Baidu NLP. Her research interests span a wide range of topics including machine translation, dialogue systems, knowledge graph, etc. She was a leading member of the machine translation project to win the second prize of the State Preeminent Science and Technology Award of China. She was the Program Co-Chair of ACL (the Association for Computational Linguistics) in 2014 and AACL in 2020 (Asia-PacificÂ Chapter of ACL). 

---
### Invited Talk 5 by [Kay-Fan Cheung](https://www.polyu.edu.hk/cbs/cts/en/people/members/58-dr-cheung-kay-fan-andrew)

<div class='container'>
<div class='row'>
	<div class='col-3'>
		<img alt="pic" src="https://www.polyu.edu.hk/cbs/web/en/img/?hash=5677e6e11998cd6d4ede04624cb20a9e.jpg">
	</div>
	<div class='col-9'>
		<b>Title</b>: Machine-aided simultaneous interpreting: An experiment  
		<br>
		<b>Abstract</b>: The talk will report the results of an experiment investigating whether technology can improve the efficiency and quality of simultaneous interpreting (SI) by human interpreters. Unfamiliar accents are one factor that can negatively affect SI performance. The real-time transcription of accented speech by automatic speech recognition (ASR) technology may aid interpreters. However, SI performance may suffer because of the additional effort needed to read the ASR transcription while juggling the multiple sub-tasks of SI.  <br>
		Twenty-four native Mandarin-speaking participants performed SI of a speech in English by a non-native speaker into Mandarin Chinese. Parts of the speech were subtitled by ASR technology while other parts were not. Raters scored the Mandarin SI renditions on two parameters: accuracy and fluency. Quantitative analysis of the scores indicated that the raters scored the subtitled parts higher for accuracy but lower for fluency than the un-subtitled parts. Qualitative analysis of post-test interviews with the participants suggested that correct ASR-generated subtitles can improve SI performance. However, having to read the subtitles, especially when incorrect, was perceived as a hindrance to the SI process.   <br>
		The data suggest that correct subtitles generated by ASR technology may improve the performance of human interpreters. The SI curriculum should incorporate training on how to use subtitles generated by ASR technology. 
	</div>
</div>
</div>

**Andrew K.F. Cheung** is Associate Professor at the Department of Chinese and Bilingual Studies of the Hong Kong Polytechnic University. He completed his MA in Conference Interpreting and Translation at the Graduate Institute of Translation and Interpreting Studies of Fu-jen Catholic University and did his Ph.D. at the University of East Anglia. His research interests include cognitive aspects of multilingual and multimodal processing, corpus-based interpreting studies, quality perception of interpreting services and pedagogy of interpreting. He is also a member of AIIC.

---
### Invited Talk 6 by [Qun Liu](https://scholar.google.com.sg/citations?user=2HhiGzcAAAAJ&hl=en)

<div class='container'>
<div class='row'>
	<div class='col-3'>
		<img alt="pic" src="http://dev3.noahlab.com.hk/images/news/qunliu.png">
	</div>
	<div class='col-9'>
		<b>Title</b>: Research and Practice of Simultaneous Machine Translation in Huawei Noah's Ark Lab  
		<br>
		<b>Abstract</b>: In this talk, I will introduce our research efforts and the development of Huawei simultaneous translation systems, both in cloud and in mobile phones.  To get a good balance between translation quality and latency, we proposed a general framework for adapting neural machine translation to translate simultaneously. To enhance the robustness of the system with regard to the speech style input and ASR errors, we introduced various data augmentation techniques including GPT-based pretraining models for paraphrasing. We further conducted optimizations to improve the run-time performance on terminal devices.  We finally obtained satisfactory performance on both platforms in the given scenarios.
	</div>
</div>
</div>

**Prof. Dr. Qun Liu** is the Chief Scientist of Speech and Language Computing in Huawei Noah's Ark Lab.  He was a Full Professor in Dublin City University and the Theme Leader of the ADAPT Centre, Ireland during July 2012 and June 2018. Before that, he was as a Professor in the Institute of Computing Technology (ICT), Chinese Academy of Sciences for 20 years, where he founded and led the ICT NLP Research Group. He obtained his B.Sc., M.Sc. and Ph.D. in computer science in the University of Science and Technology of China, Chinese Academy of Sciences, and Peking University respectively. His research interests lie in the areas of Natural Language Processing and Machine Translation.  His main academic contributions are on Chinese language processing, syntax-based statistical machine translation and neural methods for natural language processing.  He has authored or co-authored more than 300 peer-reviewed research publications, which have been cited more than 7000 times. He has supervised more than 40 students to the completion of their M.Sc. or Ph.D. degrees. 




---

